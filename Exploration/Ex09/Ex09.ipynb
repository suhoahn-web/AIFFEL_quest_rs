{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49cd0a9d-bd6b-4301-ba52-6940e21d9a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 질문 샘플: 12시 땡 !\n",
      "전처리 후 답변 샘플: 하루가 또 가네요 .\n"
     ]
    }
   ],
   "source": [
    "# Step 2. 데이터 전처리\n",
    "# 한국어는 특수문자 처리가 중요합니다. 질문(Q)과 답변(A) 쌍을 추출하고 정제하는 과정을 거칩니다.\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# [루브릭 3번: 한국어 전처리를 통해 학습 데이터셋 구축]\n",
    "def preprocess_sentence(sentence):\n",
    "    # 양끝 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "    # 구두점 앞에 공백을 추가하여 단어와 분리 (예: \"반가워요!\" -> \"반가워요 !\")\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # 한글, 숫자, 영어, 기본 구두점 제외하고 모두 제거\n",
    "    sentence = re.sub(r\"[^가-힣?.!,0-9a-zA-Z]+\", \" \", sentence)\n",
    "    return sentence.strip()\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv('~/work/transformer_chatbot/data/ChatbotData.csv')\n",
    "\n",
    "# 질문과 답변 리스트 생성 및 전처리 적용\n",
    "questions = [preprocess_sentence(q) for q in data['Q']]\n",
    "answers = [preprocess_sentence(a) for a in data['A']]\n",
    "\n",
    "print(f\"전처리 후 질문 샘플: {questions[0]}\")\n",
    "print(f\"전처리 후 답변 샘플: {answers[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288126ba-b845-425d-a8ff-e43e06a047e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe70beec-2dfe-4f51-a1bc-c3765ac31878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=chatbot.txt --model_prefix=korean_spm --vocab_size=8000\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: chatbot.txt\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: chatbot.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 23646 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=369459\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1071\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 23646 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=171230\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 18292 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 23646\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 20631\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 20631 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10757 obj=11.1054 num_tokens=40457 num_tokens/piece=3.76099\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9328 obj=10.2619 num_tokens=40559 num_tokens/piece=4.34809\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8799 obj=10.3604 num_tokens=41163 num_tokens/piece=4.67815\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8770 obj=10.3436 num_tokens=41199 num_tokens/piece=4.69772\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "# Step 3. SentencePiece 사용하기\n",
    "# 형태소 분석기 대신 Google의 SentencePiece를 사용하여 서브워드 토크나이징을 진행합니다. 이는 신조어나 복합어도 유연하게 처리할 수 있는 장점이 있습니다.\n",
    "import sentencepiece as spm\n",
    "\n",
    "# [루브릭 3번: 토크나이징 및 병렬 데이터 구축]\n",
    "# SentencePiece 학습을 위해 텍스트 파일 생성\n",
    "with open('chatbot.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in questions + answers:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "# SentencePiece 모델 학습 (어휘 사전 크기 8000)\n",
    "spm.SentencePieceTrainer.Train('--input=chatbot.txt --model_prefix=korean_spm --vocab_size=8000')\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# 시작 토큰(BOS)과 종료 토큰(EOS) 번호 정의 (보통 0, 1, 2는 예약어)\n",
    "# SentencePiece 설정에 따라 다를 수 있으나 여기서는 직접 ID를 할당하여 정수 인코딩 진행\n",
    "def encode(sentence):\n",
    "    return [s.bos_id()] + s.EncodeAsIds(sentence) + [s.eos_id()]\n",
    "\n",
    "# 모든 질문과 답변을 정수 시퀀스로 변환\n",
    "tokenized_questions = [encode(q) for q in questions]\n",
    "tokenized_answers = [encode(a) for a in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda8792b-1bfc-42c5-b4dd-6ada0025a493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.14.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-2.1.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.13.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.2.6)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.14.0)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.10.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (11.2.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.3.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.4.0-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading keras-3.13.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.10.1-py3-none-any.whl (107 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\n",
      "Downloading wrapt-2.1.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
      "Downloading rich-14.3.2-py3-none-any.whl (309 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, optree, opt_einsum, ml_dtypes, mdurl, markdown, grpcio, google_pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [tensorflow]2\u001b[0m [tensorflow]]data-server]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.4.0 astunparse-1.6.3 flatbuffers-25.12.19 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 keras-3.13.2 libclang-18.1.1 markdown-3.10.1 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.4 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 rich-14.3.2 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0 werkzeug-3.1.5 wrapt-2.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd2a0df3-58c5-4362-bad0-bd95061d2388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 완료! 데이터의 크기(shape): (11823, 40)\n",
      "✅ 데이터셋 구축 성공!\n"
     ]
    }
   ],
   "source": [
    "# Step 3-2. 패딩 작업 추가하기\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# [루브릭 3번: 데이터셋 구축을 위한 패딩 처리]\n",
    "\n",
    "# 모든 시퀀스의 길이를 MAX_LENGTH(40)으로 통일합니다.\n",
    "# 부족한 부분은 뒤(post)에 0을 채웁니다.\n",
    "tokenized_questions = pad_sequences(tokenized_questions, maxlen=MAX_LENGTH, padding='post')\n",
    "tokenized_answers = pad_sequences(tokenized_answers, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "print(\"패딩 완료! 데이터의 크기(shape):\", tokenized_questions.shape)\n",
    "\n",
    "# 이제 다시 데이터셋을 생성합니다. (에러가 났던 그 코드입니다)\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': tokenized_questions,\n",
    "        'dec_inputs': tokenized_answers[:, :-1]  # 마지막 토큰 제외 (MAX_LENGTH - 1)\n",
    "    },\n",
    "    {\n",
    "        'outputs': tokenized_answers[:, 1:]   # 시작 토큰 제외 (MAX_LENGTH - 1)\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print(\"✅ 데이터셋 구축 성공!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9b50803-9da7-417c-8b34-29b114ba5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 구축 코드 재점검\n",
    "# [루브릭 3번: 병렬 데이터 구축의 적절성 재확인]\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 텐서플로우 데이터셋으로 변환 (슬라이싱 범위 주의!)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': tokenized_questions,\n",
    "        'dec_inputs': tokenized_answers[:, :-1] # 마지막 한 칸을 자름 (시작 토큰 포함)\n",
    "    },\n",
    "    tokenized_answers[:, 1:] # 첫 번째(시작 토큰)를 자름\n",
    "))\n",
    "\n",
    "dataset = dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4e1e8d3-784e-48b8-a486-8cd4d56b19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. 모델 구성하기\n",
    "# 트랜스포머의 핵심 구조인 멀티 헤드 어텐션과 인코더, 디코더를 구현합니다.\n",
    "import tensorflow as tf\n",
    "\n",
    "# [루브릭 2번: 데이터 타입 충돌 해결 버전]\n",
    "\n",
    "# 1. 포지셔널 인코딩 (수정됨: tf.cast 추가로 데이터 타입 일치화)\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # [핵심 수정]: 입력을 확실히 float32 밀집 텐서로 변환하여 계산 에러 방지\n",
    "        inputs = tf.cast(inputs, tf.float32)\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "# --- Step 4의 나머지 부분(MultiHeadAttention, encoder_layer 등)은 이전과 동일합니다 ---\n",
    "# (코드의 가독성을 위해 생략하지만, 실제 셀에는 이전 답변의 나머지 구조가 다 있어야 합니다)\n",
    "\n",
    "def transformer(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "    \n",
    "    enc_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape=(1, 1, None), name='enc_padding_mask')(inputs)\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(create_look_ahead_mask, output_shape=(1, None, None), name='look_ahead_mask')(dec_inputs)\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape=(1, 1, None), name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # [수정]: mask_zero=False (기본값) 확인 및 데이터 타입 명시\n",
    "    enc_emb = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    enc_emb *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    enc_outputs = PositionalEncoding(vocab_size, d_model)(enc_emb)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        enc_outputs = encoder_layer(units=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=f\"encoder_layer_{i}\")(inputs=[enc_outputs, enc_padding_mask])\n",
    "\n",
    "    dec_emb = tf.keras.layers.Embedding(vocab_size, d_model)(dec_inputs)\n",
    "    dec_emb *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    dec_outputs = PositionalEncoding(vocab_size, d_model)(dec_emb)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        dec_outputs = decoder_layer(units=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=f\"decoder_layer_{i}\")(inputs=[dec_outputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9dfb5135-1b05-4953-a1bd-64ca0733730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-1\n",
    "# [루브릭 2번: 안정적인 수렴을 위한 마스킹 손실 함수]\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    # 0(패딩)인 부분은 loss 계산에서 제외하도록 마스킹\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f731b-0a4c-4b78-93f5-b86341f31889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 준비 완료! 이제 50회 학습을 시작합니다. 숫자가 올라가는지 확인해주세요.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 06:12:01.613396: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 524288000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/185\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:14\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 1.9802"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 06:12:02.992467: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 524288000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  2/185\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:55\u001b[0m 1s/step - accuracy: 0.0000e+00 - loss: 1.9514 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 06:12:04.283612: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 524288000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  3/185\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:51\u001b[0m 1s/step - accuracy: 0.0000e+00 - loss: 1.9466"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 06:12:05.497171: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 524288000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  4/185\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:44\u001b[0m 1s/step - accuracy: 0.0000e+00 - loss: 1.9386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 06:12:06.656809: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 524288000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 78/185\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 1s/step - accuracy: 0.0097 - loss: 1.8505"
     ]
    }
   ],
   "source": [
    "# [루브릭 2번: 모델 컴파일 및 50회 학습 진행]\n",
    "\n",
    "# 1. 학습률 스케줄러 설정 (D_MODEL=256 기준)\n",
    "# D_MODEL이 선언되지 않았을 경우를 대비해 256으로 고정합니다.\n",
    "D_MODEL = 256\n",
    "learning_rate = CustomSchedule(D_MODEL, warmup_steps=1000) \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# 2. 모델 컴파일 (공부 방법 설정)\n",
    "# loss_function이 메모리에 있어야 합니다. \n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "print(\"✅ 모델 준비 완료! 이제 50회 학습을 시작합니다. 숫자가 올라가는지 확인해주세요.\")\n",
    "\n",
    "# 3. 진짜 학습 시작\n",
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS)\n",
    "\n",
    "print(\"✅ 50회 학습이 모두 완료되었습니다! 이제 Step 5 셀을 실행해서 답변을 확인해보세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7412dbec-e717-45b7-9bc5-1229e926da34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇 테스트를 시작합니다.\n",
      "질문: 안녕하세요 -> 답변: 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심 점심\n",
      "질문: 배고파 -> 답변: 되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠되겠죠\n",
      "질문: 고민이 있어 -> 답변: 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골 골\n",
      "질문: 오늘 날씨 어때? -> 답변: 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상 현상\n"
     ]
    }
   ],
   "source": [
    "# Step 5. 모델 평가하기\n",
    "# 이제 학습된 모델이 사용자의 질문에 답변을 생성하는 예측 함수를 만듭니다.\n",
    "# [루브릭 1번: 한국어 입력에 대해 한국어로 답변하는 함수 구현]\n",
    "\n",
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 1. 인코더 입력 생성 (정수 인코딩 + 패딩)\n",
    "    enc_input = [s.bos_id()] + s.EncodeAsIds(sentence) + [s.eos_id()]\n",
    "    enc_input = pad_sequences([enc_input], maxlen=MAX_LENGTH, padding='post')\n",
    "    \n",
    "    # 2. 디코더 입력 초기화 (시작 토큰 주입)\n",
    "    output_sequence = tf.expand_dims([s.bos_id()], 0)\n",
    "\n",
    "    # [중요] 모든 입력을 텐서 형태로 통일하여 ValueError 방지\n",
    "    enc_input = tf.convert_to_tensor(enc_input, dtype=tf.int32)\n",
    "    output_sequence = tf.cast(output_sequence, tf.int32)\n",
    "\n",
    "    # 3. 단어 생성 루프\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 모델 예측 (인코더 입력과 현재까지의 디코더 입력 전달)\n",
    "        predictions = model(inputs=[enc_input, output_sequence], training=False)\n",
    "        \n",
    "        # 마지막 타임스텝의 결과만 추출\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        \n",
    "        # 가장 높은 확률을 가진 단어 ID 선택\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 종료 토큰(</s>)이 나오면 예측 중단\n",
    "        if tf.equal(predicted_id, s.eos_id()):\n",
    "            break\n",
    "\n",
    "        # 생성된 단어를 디코더 입력 시퀀스에 결합\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    # 4. 정수 시퀀스를 다시 문장으로 변환\n",
    "    result = s.DecodeIds(output_sequence.numpy().squeeze().tolist())\n",
    "    return result\n",
    "\n",
    "# --- 테스트 코드 ---\n",
    "print(\"챗봇 테스트를 시작합니다.\")\n",
    "questions_test = ['안녕하세요', '배고파', '고민이 있어', '오늘 날씨 어때?']\n",
    "\n",
    "for q in questions_test:\n",
    "    print(f\"질문: {q} -> 답변: {decoder_inference(q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ce1037b-5120-4d9b-8ad0-0c5794897d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터셋이 새롭게 구축되었습니다. 이제 학습을 다시 시도해보세요!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f19c2c-c665-4cc1-8649-73d1fde72f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
