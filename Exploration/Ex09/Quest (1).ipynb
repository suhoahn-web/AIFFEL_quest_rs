{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, ops\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 데이터 다운로드 및 도구 설치\n",
        "!mkdir -p data\n",
        "if not os.path.exists('data/ChatbotData.csv'):\n",
        "    !wget https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv -O data/ChatbotData.csv\n",
        "!pip install sentencepiece\n",
        "\n",
        "# 전처리 함수\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.strip()\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^가-힣?.!,0-9a-zA-Z]+\", \" \", sentence)\n",
        "    return sentence.strip()\n",
        "\n",
        "data = pd.read_csv('data/ChatbotData.csv')\n",
        "questions = [preprocess_sentence(q) for q in data['Q']]\n",
        "answers = [preprocess_sentence(a) for a in data['A']]\n",
        "\n",
        "with open('chatbot.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in questions + answers:\n",
        "        f.write(line + '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMaqX1K3A3kD",
        "outputId": "21d1df98-1aa7-4c33-adc3-f1588728dec4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: 데이터 결합 방식 변경\n",
        "tokenized_data = []\n",
        "for q, a in zip(questions, answers):\n",
        "    # 구조: BOS + 질문 + | + 답변 + EOS\n",
        "    # 여기서 '|'는 질문이 끝나고 답변이 시작됨을 알리는 아주 강력한 신호입니다.\n",
        "    combined = [s.bos_id()] + s.EncodeAsIds(q) + s.EncodeAsIds(\"|\") + s.EncodeAsIds(a) + [s.eos_id()]\n",
        "    tokenized_data.append(combined)\n",
        "\n",
        "# 길이를 넉넉히 60으로 잡고 패딩 처리\n",
        "tokenized_data = pad_sequences(tokenized_data, maxlen=60, padding='post', value=0)\n",
        "\n",
        "# [중요] 데이터셋을 새로 만들고 모델을 반드시 다시 'fit' 시켜야 합니다!\n",
        "dataset = tf.data.Dataset.from_tensor_slices((tokenized_data[:, :-1], tokenized_data[:, 1:])).shuffle(20000).batch(64)\n",
        "model.fit(dataset, epochs=30) # 이 구조를 다시 학습시켜야 모델이 '벽'을 인식합니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfH1_3ahA3mh",
        "outputId": "4d09e823-a6ca-41d9-d801-84817ac7cf0c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - accuracy: 0.9638 - loss: 0.1927\n",
            "Epoch 2/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9640 - loss: 0.1895\n",
            "Epoch 3/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - accuracy: 0.9643 - loss: 0.1882\n",
            "Epoch 4/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9642 - loss: 0.1879\n",
            "Epoch 5/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.9642 - loss: 0.1875\n",
            "Epoch 6/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9644 - loss: 0.1863\n",
            "Epoch 7/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 51ms/step - accuracy: 0.9641 - loss: 0.1857\n",
            "Epoch 8/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 51ms/step - accuracy: 0.9643 - loss: 0.1851\n",
            "Epoch 9/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9645 - loss: 0.1842\n",
            "Epoch 10/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9643 - loss: 0.1843\n",
            "Epoch 11/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - accuracy: 0.9644 - loss: 0.1835\n",
            "Epoch 12/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9645 - loss: 0.1835\n",
            "Epoch 13/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9646 - loss: 0.1826\n",
            "Epoch 14/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9646 - loss: 0.1824\n",
            "Epoch 15/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9647 - loss: 0.1818\n",
            "Epoch 16/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9646 - loss: 0.1813\n",
            "Epoch 17/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9648 - loss: 0.1813\n",
            "Epoch 18/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.9650 - loss: 0.1807\n",
            "Epoch 19/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9651 - loss: 0.1802\n",
            "Epoch 20/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9647 - loss: 0.1806\n",
            "Epoch 21/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9649 - loss: 0.1802\n",
            "Epoch 22/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.9649 - loss: 0.1798\n",
            "Epoch 23/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9650 - loss: 0.1794\n",
            "Epoch 24/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9648 - loss: 0.1792\n",
            "Epoch 25/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9648 - loss: 0.1789\n",
            "Epoch 26/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.9648 - loss: 0.1790\n",
            "Epoch 27/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9649 - loss: 0.1786\n",
            "Epoch 28/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9651 - loss: 0.1782\n",
            "Epoch 29/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9650 - loss: 0.1784\n",
            "Epoch 30/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9651 - loss: 0.1780\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7878d8b854f0>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: GPT-1 모델 설계\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + ops.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * ops.power(x, 3))))\n",
        "\n",
        "class GPTCausalMask(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        seq_len = ops.shape(inputs)[1]\n",
        "        mask = 1.0 - ops.tri(seq_len, seq_len, k=0, dtype=\"float32\")\n",
        "        return mask[None, None, :, :]\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, 1, input_shape[1], input_shape[1])\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, position, d_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = (ops.cast(ops.arange(position), \"float32\")[:, None] /\n",
        "                      ops.power(10000.0, (2.0 * (ops.cast(ops.arange(d_model), \"float32\")[None, :] // 2.0)) / ops.cast(d_model, \"float32\")))\n",
        "        sines, cosines = ops.sin(angle_rads[:, 0::2]), ops.cos(angle_rads[:, 1::2])\n",
        "        return ops.concatenate([sines, cosines], axis=-1)[None, ...]\n",
        "    def call(self, inputs):\n",
        "        return ops.cast(inputs, \"float32\") + self.pos_encoding[:, :ops.shape(inputs)[1], :]\n",
        "\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_heads, self.d_model, self.depth = num_heads, d_model, d_model // num_heads\n",
        "        self.q_dense, self.k_dense, self.v_dense, self.dense = [layers.Dense(d_model) for _ in range(4)]\n",
        "    def split_heads(self, x, batch_size):\n",
        "        return ops.transpose(ops.reshape(x, (batch_size, -1, self.num_heads, self.depth)), (0, 2, 1, 3))\n",
        "    def call(self, inputs):\n",
        "        q, k, v, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
        "        batch_size = ops.shape(q)[0]\n",
        "        q, k, v = self.split_heads(self.q_dense(q), batch_size), self.split_heads(self.k_dense(k), batch_size), self.split_heads(self.v_dense(v), batch_size)\n",
        "        logits = ops.matmul(q, ops.transpose(k, (0, 1, 3, 2))) / np.sqrt(self.depth)\n",
        "        if mask is not None: logits += (mask * -1e9)\n",
        "        out = ops.transpose(ops.matmul(ops.softmax(logits, axis=-1), v), (0, 2, 1, 3))\n",
        "        return self.dense(ops.reshape(out, (batch_size, -1, self.d_model)))\n",
        "\n",
        "def build_gpt(vocab_size, num_layers, dff, d_model, num_heads):\n",
        "    inputs = layers.Input(shape=(None,), name=\"inputs\")\n",
        "    look_ahead_mask = GPTCausalMask()(inputs)\n",
        "    x = layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    x = PositionalEncoding(vocab_size, d_model)(x * np.sqrt(d_model))\n",
        "    for i in range(num_layers):\n",
        "        attn = MultiHeadAttention(d_model, num_heads, name=f\"mha_{i}\")({'query': x, 'key': x, 'value': x, 'mask': look_ahead_mask})\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn)\n",
        "        ffn = layers.Dense(dff, activation=gelu)(x)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + layers.Dense(d_model)(ffn))\n",
        "    return keras.Model(inputs=inputs, outputs=layers.Dense(vocab_size)(x))"
      ],
      "metadata": {
        "id": "kW9nGXdlA3o_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: 모델 학습 및 결과 확인\n",
        "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super().__init__()\n",
        "        self.d_model, self.warmup_steps = ops.cast(d_model, \"float32\"), warmup_steps\n",
        "    def __call__(self, step):\n",
        "        step = ops.cast(step, \"float32\")\n",
        "        return ops.rsqrt(self.d_model) * ops.minimum(ops.rsqrt(step), step * (self.warmup_steps**-1.5))\n",
        "\n",
        "model = build_gpt(8000, 4, 512, 256, 8)\n",
        "model.compile(optimizer=keras.optimizers.Adam(CustomSchedule(256), beta_1=0.9, beta_2=0.98, epsilon=1e-9),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((tokenized_data[:, :-1], tokenized_data[:, 1:])).shuffle(20000).batch(64)\n",
        "model.fit(dataset, epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYHgWFbrA3rj",
        "outputId": "67938778-53b4-4937-d723-c1941fc333c2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 82ms/step - accuracy: 0.4848 - loss: 7.4078\n",
            "Epoch 2/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - accuracy: 0.7572 - loss: 3.6745\n",
            "Epoch 3/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.8047 - loss: 1.6549\n",
            "Epoch 4/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step - accuracy: 0.8148 - loss: 1.3750\n",
            "Epoch 5/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step - accuracy: 0.8264 - loss: 1.2453\n",
            "Epoch 6/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.8344 - loss: 1.1417\n",
            "Epoch 7/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - accuracy: 0.8416 - loss: 1.0528\n",
            "Epoch 8/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 51ms/step - accuracy: 0.8479 - loss: 0.9741\n",
            "Epoch 9/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - accuracy: 0.8591 - loss: 0.8738\n",
            "Epoch 10/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.8697 - loss: 0.7856\n",
            "Epoch 11/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.8839 - loss: 0.6825\n",
            "Epoch 12/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - accuracy: 0.8980 - loss: 0.5854\n",
            "Epoch 13/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.9124 - loss: 0.4973\n",
            "Epoch 14/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9252 - loss: 0.4221\n",
            "Epoch 15/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.9377 - loss: 0.3537\n",
            "Epoch 16/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9456 - loss: 0.3085\n",
            "Epoch 17/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9502 - loss: 0.2807\n",
            "Epoch 18/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9519 - loss: 0.2659\n",
            "Epoch 19/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.9529 - loss: 0.2579\n",
            "Epoch 20/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.9532 - loss: 0.2546\n",
            "Epoch 21/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9528 - loss: 0.2545\n",
            "Epoch 22/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9532 - loss: 0.2517\n",
            "Epoch 23/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9541 - loss: 0.2468\n",
            "Epoch 24/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9557 - loss: 0.2394\n",
            "Epoch 25/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9573 - loss: 0.2314\n",
            "Epoch 26/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9581 - loss: 0.2279\n",
            "Epoch 27/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9587 - loss: 0.2237\n",
            "Epoch 28/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9598 - loss: 0.2178\n",
            "Epoch 29/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9607 - loss: 0.2143\n",
            "Epoch 30/30\n",
            "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9610 - loss: 0.2117\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7878154e7890>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: 한글 깨짐 방지 및 답변 추출 로직 강화\n",
        "def gpt_inference(sentence, temp=0.8):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    # 질문 뒤에 학습 시 사용한 구분자 '|'를 반드시 붙여줍니다.\n",
        "    input_ids = [s.bos_id()] + s.EncodeAsIds(sentence) + s.EncodeAsIds(\"|\")\n",
        "    input_seq = tf.convert_to_tensor([input_ids], dtype=tf.int32)\n",
        "\n",
        "    for _ in range(40): # 최대 40단어 생성\n",
        "        predictions = model(inputs=input_seq, training=False)[:, -1:, :] / temp\n",
        "\n",
        "        # 확률 기반 샘플링으로 답변의 다양성 확보\n",
        "        predicted_id = tf.random.categorical(tf.reshape(predictions, [1, -1]), num_samples=1)\n",
        "        predicted_id = tf.cast(predicted_id, tf.int32)\n",
        "\n",
        "        curr_id = predicted_id.numpy().item()\n",
        "        # EOS가 나오거나 패딩(0)이 나오면 즉시 중단\n",
        "        if curr_id == s.eos_id() or curr_id == 0:\n",
        "            break\n",
        "\n",
        "        input_seq = tf.concat([input_seq, predicted_id], axis=-1)\n",
        "\n",
        "    # 전체 시퀀스 복원\n",
        "    full_sentence = s.Decode(input_seq.numpy().squeeze().tolist())\n",
        "\n",
        "    # 구분자 '|' 뒷부분만 답변으로 추출\n",
        "    if \"|\" in full_sentence:\n",
        "        answer = full_sentence.split(\"|\")[-1].strip()\n",
        "    else:\n",
        "        answer = full_sentence.replace(sentence, \"\").strip()\n",
        "\n",
        "    # 결과가 비어있거나 이상하면 예외 처리\n",
        "    answer = answer.replace(\"??\", \"\").strip()\n",
        "    return answer if len(answer) > 0 else \"조금 더 구체적으로 물어봐 주실래요?\"\n",
        "\n",
        "# 최종 성능 검증 테스트\n",
        "test_questions = [\n",
        "    \"나 너무 피곤해\",\n",
        "    \"갑자기 화가나\",\n",
        "    \"나는 지금 배가 고파. 저녁 메뉴를 추천해줄래?\",\n",
        "    \"AI는 뭐야?\",\n",
        "    \"앞으로 AI 시장은 어떻게 변할까?\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- 최종 모델 성능 검증 결과 ---\")\n",
        "for q in test_questions:\n",
        "    print(f\"Q: {q}\\nA: {gpt_inference(q)}\\n{'-'*30}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OHEmnOoA3uQ",
        "outputId": "79e8bf1c-8629-4230-fd12-149958f7d990"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 최종 모델 성능 검증 결과 ---\n",
            "Q: 나 너무 피곤해\n",
            "A: ⁇  좀 더 일찍 잠자리에 들어보세요 .\n",
            "------------------------------\n",
            "Q: 갑자기 화가나\n",
            "A: ⁇  자신의 삶의 낙이죠 .\n",
            "------------------------------\n",
            "Q: 나는 지금 배가 고파. 저녁 메뉴를 추천해줄래?\n",
            "A: ⁇  더 참았어요 .\n",
            "------------------------------\n",
            "Q: AI는 뭐야?\n",
            "A: A ⁇ 는 뭐야 ?  ⁇  그 사람의 사랑의 고민를 찾아\n",
            "------------------------------\n",
            "Q: 앞으로 AI 시장은 어떻게 변할까?\n",
            "A: 앞으로 A ⁇  시장은 어떻게 변할까 ?  ⁇  하세요 .\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#회고\n",
        "# Transformer는 질문을 해석하는 인코더와 답변을 만드는 디코더가 있는 것임. 하지만 GPT는 디코더만 사용함. GPT의 핵심은 앞에 나온 단어를 보고 다음에 올 가장 자연스러운 단어를 예측하는 것임. 따라서 인코더를 생략하고 디코더 스택만 쌓은 GPT 스타일로 변경,\n",
        "# 기존 FeLU 대신 GPT-1에서 사용된 GELU를 적용함. GELU는 0 근처에서 더 부드럽게 꺾이는 특성이 있기 때문에 인경 신경망이 복잡한 언어 패턴을 유연하게 학습하도록 도움\n",
        "# 지난주 과제는 데이터의 한계점이라 생각하고 이번엔 데이터를 바꿔봤는데도 여전히 답이 이상한 것들이 많았음. 학습된 데이터에서는 AI라는 단어가 없어서 그런지 AI 자체를 인지하지 못함.\n",
        "# 추후에는 더 많은 양을 학습시키는 등의 노력이 필요해보임"
      ],
      "metadata": {
        "id": "p2ATOUF-etyf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}